{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BzcPlUTmtImx"
   },
   "source": [
    "# Commerce Challenge 1 - Baseline Submission\n",
    "\n",
    "This notebook provides a simple baseline for **Commerce Challenge 1: Weekly Store-SKU Demand Forecasting**.\n",
    "\n",
    "**Goal**: Predict `units_sold_next_week` for each SKU-week combination\n",
    "**Metric**: Root Mean Squared Error (RMSE) - Lower is better\n",
    "\n",
    "## Instructions:\n",
    "1. **Replace API credentials** in the first cell with your team's API key and name\n",
    "2. **Run all cells** to generate and submit baseline predictions\n",
    "3. **Check the output** for your submission score\n",
    "\n",
    "This baseline uses only tabular sales data with a simple Random Forest regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentds import BenchmarkClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 387
    },
    "executionInfo": {
     "elapsed": 7017,
     "status": "error",
     "timestamp": 1760999018030,
     "user": {
      "displayName": "Lily Wang",
      "userId": "09956471088063250847"
     },
     "user_tz": 240
    },
    "id": "yJcAdcDctIov",
    "outputId": "e0046e7e-ab5b-46f5-c030-f6fb0cc9069c"
   },
   "outputs": [],
   "source": [
    "# 1. Initialize Client and Load Data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# from agentds import BenchmarkClient\n",
    "\n",
    "# # üîë REPLACE WITH YOUR CREDENTIALS\n",
    "# client = BenchmarkClient(\n",
    "#     api_key=\"adsb_AJp9fgtjuTRZ4Tk0mNnYDWxG_1759039055\",        # Get from your team dashboard\n",
    "#     team_name=\"clai\"     # Your exact team name\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîë REPLACE WITH YOUR CREDENTIALS\n",
    "client = BenchmarkClient(\n",
    "    api_key=\"adsb_AJp9fgtjuTRZ4Tk0mNnYDWxG_1759039055\",        # Get from your team dashboard\n",
    "    team_name=\"clai\"     # Your exact team name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-0w0WvOwtkwC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66000,\n",
       " 12000,\n",
       "    sku_id  week  units_sold  price  promo_flag\n",
       " 0       1     1          51   0.72           0\n",
       " 1       1     2          41   0.70           0,\n",
       "    week                                  event_description\n",
       " 0     1  No large gatherings or city events near the st...\n",
       " 1     2  Regular trading week with no special events sc...,\n",
       "    sku_id   category subtype  base_price\n",
       " 0       1  Beverages   water        0.69\n",
       " 1       2  Beverages   water        0.69)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell A ‚Äî Load CSVs (and optional small preview)\n",
    "\n",
    "# File names assume you're in the same folder as the CSVs;\n",
    "# adjust paths if needed.\n",
    "train_sales   = pd.read_csv(\"./agent_ds_commerce/sales_history_train.csv\")\n",
    "test_sales    = pd.read_csv(\"./agent_ds_commerce/sales_history_test.csv\")\n",
    "events_df     = pd.read_csv(\"./agent_ds_commerce/store_events.csv\")\n",
    "products_df   = pd.read_csv(\"./agent_ds_commerce/products.csv\")\n",
    "\n",
    "# Ensure core integer types for keys/flags --> convert to int\n",
    "for df in (train_sales, test_sales):\n",
    "    for c in [\"sku_id\", \"week\", \"promo_flag\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].astype(int)\n",
    "\n",
    "# (Optional) sanity peek to understand your earlier question about price/promos\n",
    "# sku1_preview = train_sales.query(\"sku_id == 1\").sort_values(\"week\").head(40)\n",
    "# sku1_preview\n",
    "len(train_sales), len(test_sales), train_sales.head(2), events_df.head(2), products_df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shend\\AppData\\Local\\Temp\\ipykernel_34252\\4147603284.py:79: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train_feat = train.groupby(\"sku_id\", group_keys=False).apply(build_ts_feats_train)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((64500, 28), (64500,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell B ‚Äî TRAIN features (leak-safe) + label (t+1) + local NLP event-strength\n",
    "import numpy as np\n",
    "from typing import Tuple, Dict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Week cyclic features\n",
    "# ---------------------------\n",
    "def add_week_cycles(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"week_sin\"] = np.sin(2*np.pi*df[\"week\"]/52.0)\n",
    "    df[\"week_cos\"] = np.cos(2*np.pi*df[\"week\"]/52.0)\n",
    "    return df\n",
    "\n",
    "train = add_week_cycles(train_sales)\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Products join (safe)\n",
    "# ---------------------------\n",
    "prod = products_df.copy()\n",
    "\n",
    "# Make sure columns exist with correct types\n",
    "if \"category\" in prod.columns:\n",
    "    prod[\"category\"] = prod[\"category\"].astype(str).fillna(\"unknown\")\n",
    "else:\n",
    "    prod[\"category\"] = \"unknown\"\n",
    "\n",
    "if \"brand\" in prod.columns:\n",
    "    prod[\"brand\"] = prod[\"brand\"].astype(str).fillna(\"unknown\")\n",
    "else:\n",
    "    prod[\"brand\"] = \"unknown\"\n",
    "\n",
    "if \"base_price\" not in prod.columns:\n",
    "    prod[\"base_price\"] = np.nan\n",
    "\n",
    "train = train.merge(prod[[\"sku_id\",\"category\",\"brand\",\"base_price\"]], on=\"sku_id\", how=\"left\")\n",
    "\n",
    "# ---------------------------\n",
    "# 3) Signed discount in [-1, +1]\n",
    "# ---------------------------\n",
    "def discount_pct_signed(base_price, price) -> float:\n",
    "    \"\"\"\n",
    "    Signed: + means markdown vs base; - means markup vs base.\n",
    "    Clips to [-1, +1]; returns 0.0 when base_price invalid.\n",
    "    \"\"\"\n",
    "    if base_price is None or np.isnan(base_price) or base_price <= 0 or price is None or np.isnan(price):\n",
    "        return 0.0\n",
    "    val = (base_price - price) / base_price\n",
    "    return float(np.clip(val, -1.0, 1.0))\n",
    "\n",
    "train[\"discount_pct\"] = [discount_pct_signed(bp, p) for bp, p in zip(train[\"base_price\"], train[\"price\"])]\n",
    "\n",
    "# ---------------------------\n",
    "# 4) Global weekly total (train)\n",
    "# ---------------------------\n",
    "weekly_total = (\n",
    "    train.groupby(\"week\", as_index=False)[\"units_sold\"]\n",
    "    .sum().rename(columns={\"units_sold\":\"total_units_week\"})\n",
    ")\n",
    "weekly_total[\"total_units_week_lag1\"] = weekly_total[\"total_units_week\"].shift(1)\n",
    "\n",
    "# ---------------------------\n",
    "# 5) Per-SKU time-series features (past-only)\n",
    "# ---------------------------\n",
    "def build_ts_feats_train(g: pd.DataFrame) -> pd.DataFrame:\n",
    "    g = g.sort_values(\"week\").copy()  # g is per SKU group\n",
    "    # short lags for strong short-term memory\n",
    "    for L in [1,2,3]:\n",
    "        g[f\"lag{L}_units\"] = g[\"units_sold\"].shift(L)\n",
    "    # short rolling mean for stabilization\n",
    "    g[\"roll3_mean_units\"] = g[\"units_sold\"].shift(1).rolling(3, min_periods=1).mean()\n",
    "    # price/promo memory\n",
    "    g[\"lag1_price\"] = g[\"price\"].shift(1)\n",
    "    g[\"price_change_pct\"] = (g[\"price\"] - g[\"lag1_price\"]) / g[\"lag1_price\"]\n",
    "    g[\"lag1_promo\"] = g[\"promo_flag\"].shift(1)\n",
    "    return g\n",
    "\n",
    "train_feat = train.groupby(\"sku_id\", group_keys=False).apply(build_ts_feats_train)\n",
    "train_feat = train_feat.merge(weekly_total[[\"week\",\"total_units_week_lag1\"]], on=\"week\", how=\"left\")\n",
    "\n",
    "# ---------------------------\n",
    "# 6) Local NLP model: learn event strength (continuous in [-1, +1] + bucket)\n",
    "#     - train on week-t text to predict week-(t+1) total uplift vs rolling baseline\n",
    "# ---------------------------\n",
    "events = events_df.copy()\n",
    "events[\"event_description\"] = events[\"event_description\"].fillna(\"\").astype(str)\n",
    "\n",
    "def fit_event_strength_model(\n",
    "    train_df: pd.DataFrame,\n",
    "    events_df: pd.DataFrame,\n",
    "    roll_window: int = 3,\n",
    "    alpha: float = 2.0,\n",
    "    low_q: float = 0.33,\n",
    "    high_q: float = 0.67,\n",
    ") -> Tuple[pd.DataFrame, Dict[str,float], TfidfVectorizer, Ridge]:\n",
    "    # weekly totals in train\n",
    "    wk = train_df.groupby(\"week\", as_index=False)[\"units_sold\"].sum().rename(columns={\"units_sold\":\"weekly_total\"})\n",
    "    wk = wk.sort_values(\"week\").reset_index(drop=True)\n",
    "    # label = next week's total\n",
    "    wk[\"weekly_total_next\"] = wk[\"weekly_total\"].shift(-1)\n",
    "    # rolling baseline for the LABEL (past-only relative to that label)\n",
    "    baseline = wk[\"weekly_total_next\"].shift(1).rolling(roll_window, min_periods=1).mean()\n",
    "    wk[\"uplift\"] = (wk[\"weekly_total_next\"] - baseline) / baseline\n",
    "    wk[\"uplift\"] = wk[\"uplift\"].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "    wk[\"uplift\"] = wk[\"uplift\"].clip(-1.0, 1.0)\n",
    "\n",
    "    # merge week-t text\n",
    "    data = wk.merge(events_df[[\"week\",\"event_description\"]], on=\"week\", how=\"left\")\n",
    "    mask = data[\"uplift\"].notna()\n",
    "\n",
    "    # TF-IDF + Ridge\n",
    "    tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=2, max_features=5000)\n",
    "    X = tfidf.fit_transform(data.loc[mask, \"event_description\"])\n",
    "    y = data.loc[mask, \"uplift\"].astype(float)\n",
    "    ridge = Ridge(alpha=alpha, random_state=42)\n",
    "    ridge.fit(X, y)\n",
    "\n",
    "    # predict for ALL weeks in events_df (train + test weeks)\n",
    "    X_all = tfidf.transform(data[\"event_description\"])\n",
    "    data[\"event_strength_pred\"] = np.clip(ridge.predict(X_all), -1.0, 1.0)\n",
    "\n",
    "    # thresholds based on TRAIN predictions only\n",
    "    train_preds = data.loc[mask, \"event_strength_pred\"]\n",
    "    lo_thr = float(train_preds.quantile(low_q))\n",
    "    hi_thr = float(train_preds.quantile(high_q))\n",
    "\n",
    "    def bucketize(v):\n",
    "        if v <= lo_thr:  return \"weak\"\n",
    "        if v >= hi_thr:  return \"strong\"\n",
    "        return \"mild\"\n",
    "\n",
    "    data[\"event_strength_bucket\"] = data[\"event_strength_pred\"].apply(bucketize)\n",
    "\n",
    "    out = data[[\"week\",\"event_strength_pred\",\"event_strength_bucket\"]].copy()\n",
    "    return out, {\"lo_thr\": lo_thr, \"hi_thr\": hi_thr}, tfidf, ridge\n",
    "\n",
    "event_strength_all, evt_thr, event_tfidf, event_ridge = fit_event_strength_model(train, events)\n",
    "\n",
    "# merge event strength into TRAIN features\n",
    "train_feat = train_feat.merge(event_strength_all, on=\"week\", how=\"left\")\n",
    "\n",
    "# ---------------------------\n",
    "# 7) Make label = units at (t+1) for same SKU\n",
    "# ---------------------------\n",
    "y_next = train[[\"sku_id\",\"week\",\"units_sold\"]].copy()\n",
    "y_next[\"week\"] = y_next[\"week\"] - 1\n",
    "y_next = y_next.rename(columns={\"units_sold\":\"units_sold_next_week\"})\n",
    "train_rows = train_feat.merge(y_next, on=[\"sku_id\",\"week\"], how=\"inner\")\n",
    "\n",
    "# ---------------------------\n",
    "# 8) Build TRAIN matrices (encode category/brand top-K)\n",
    "# ---------------------------\n",
    "def topk_one_hot(train_series: pd.Series, test_series: pd.Series, k=30, prefix=\"x\"):\n",
    "    top = train_series.value_counts().index[:k]\n",
    "    train_c = pd.Categorical(train_series, categories=top)\n",
    "    test_c  = pd.Categorical(test_series,  categories=top)\n",
    "    tr = pd.get_dummies(train_c, prefix=prefix)\n",
    "    te = pd.get_dummies(test_c,  prefix=prefix)\n",
    "    te = te.reindex(columns=tr.columns, fill_value=0)\n",
    "    return tr, te  # we'll build TEST later, but need the aligned columns\n",
    "\n",
    "# Numeric columns (include event strength)\n",
    "num_cols = [\n",
    "    \"price\",\"promo_flag\",\"discount_pct\",\n",
    "    \"week_sin\",\"week_cos\",\n",
    "    \"lag1_units\",\"lag2_units\",\"lag3_units\",\n",
    "    \"roll3_mean_units\",\n",
    "    \"lag1_price\",\"price_change_pct\",\"lag1_promo\",\n",
    "    \"total_units_week_lag1\",\n",
    "    \"event_strength_pred\",   # continuous learned signal\n",
    "]\n",
    "\n",
    "# Bucket as one-hots (optional but useful)\n",
    "evt_tr = pd.get_dummies(train_rows[\"event_strength_bucket\"].fillna(\"mild\"), prefix=\"evt\")\n",
    "\n",
    "# Product one-hots (derive TRAIN-side encoders here; use aligned cols for TEST later)\n",
    "cat_tr, _ = topk_one_hot(train_rows[\"category\"], train_rows[\"category\"], k=30, prefix=\"category\")\n",
    "br_tr,  _ = topk_one_hot(train_rows[\"brand\"],    train_rows[\"brand\"],    k=30, prefix=\"brand\")\n",
    "\n",
    "X_train_num = train_rows[num_cols].astype(float).fillna(0)\n",
    "X_train = pd.concat([X_train_num.reset_index(drop=True),\n",
    "                     evt_tr.reset_index(drop=True),\n",
    "                     cat_tr.reset_index(drop=True),\n",
    "                     br_tr.reset_index(drop=True)], axis=1)\n",
    "\n",
    "y_train = train_rows[\"units_sold_next_week\"].astype(float)\n",
    "\n",
    "X_train.shape, y_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shend\\AppData\\Local\\Temp\\ipykernel_34252\\2367427856.py:30: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  comb_feat = comb_all.groupby(\"sku_id\", group_keys=False).apply(build_ts_feats_comb)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((64500, 28), (12000, 28))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell C ‚Äî TEST features (uses fitted TF-IDF/Ridge + thresholds from Cell B)\n",
    "\n",
    "# Reuse: products_df, events_df, weekly_total (from train), event_tfidf, event_ridge, evt_thr\n",
    "\n",
    "# 1) Prep TEST base table\n",
    "test = add_week_cycles(test_sales.copy())\n",
    "\n",
    "# Safe product join\n",
    "test = test.merge(prod[[\"sku_id\",\"category\",\"brand\",\"base_price\"]], on=\"sku_id\", how=\"left\")\n",
    "test[\"discount_pct\"] = [discount_pct_signed(bp, p) for bp, p in zip(test[\"base_price\"], test[\"price\"])]\n",
    "\n",
    "# 2) Recompute per-SKU lags on (train ‚à™ test with units_sold=NaN)\n",
    "comb = train[[\"sku_id\",\"week\",\"units_sold\",\"price\",\"promo_flag\",\"week_sin\",\"week_cos\",\n",
    "              \"category\",\"brand\",\"base_price\",\"discount_pct\"]].copy()\n",
    "stub = test[[\"sku_id\",\"week\",\"price\",\"promo_flag\",\"week_sin\",\"week_cos\",\n",
    "             \"category\",\"brand\",\"base_price\",\"discount_pct\"]].copy()\n",
    "stub[\"units_sold\"] = np.nan\n",
    "comb_all = pd.concat([comb, stub], ignore_index=True)\n",
    "\n",
    "def build_ts_feats_comb(g: pd.DataFrame) -> pd.DataFrame:\n",
    "    g = g.sort_values(\"week\").copy()\n",
    "    for L in [1,2,3]:\n",
    "        g[f\"lag{L}_units\"] = g[\"units_sold\"].shift(L)\n",
    "    g[\"roll3_mean_units\"] = g[\"units_sold\"].shift(1).rolling(3, min_periods=1).mean()\n",
    "    g[\"lag1_price\"] = g[\"price\"].shift(1)\n",
    "    g[\"price_change_pct\"] = (g[\"price\"] - g[\"lag1_price\"]) / g[\"lag1_price\"]\n",
    "    g[\"lag1_promo\"] = g[\"promo_flag\"].shift(1)\n",
    "    return g\n",
    "\n",
    "comb_feat = comb_all.groupby(\"sku_id\", group_keys=False).apply(build_ts_feats_comb)\n",
    "\n",
    "# Add global lag and EVENT STRENGTH\n",
    "comb_feat = comb_feat.merge(weekly_total[[\"week\",\"total_units_week_lag1\"]], on=\"week\", how=\"left\")\n",
    "\n",
    "# Predict event strength for all weeks using fitted TF-IDF/Ridge\n",
    "events_all = events_df.copy()\n",
    "events_all[\"event_description\"] = events_all[\"event_description\"].fillna(\"\").astype(str)\n",
    "\n",
    "X_evt = event_tfidf.transform(events_all[\"event_description\"])\n",
    "evt_pred = np.clip(event_ridge.predict(X_evt), -1.0, 1.0)\n",
    "\n",
    "lo_thr, hi_thr = evt_thr[\"lo_thr\"], evt_thr[\"hi_thr\"]\n",
    "def bucketize(v):\n",
    "    if v <= lo_thr:  return \"weak\"\n",
    "    if v >= hi_thr:  return \"strong\"\n",
    "    return \"mild\"\n",
    "\n",
    "events_all = events_all[[\"week\"]].copy().assign(\n",
    "    event_strength_pred = evt_pred,\n",
    "    event_strength_bucket = [bucketize(v) for v in evt_pred]\n",
    ")\n",
    "\n",
    "comb_feat = comb_feat.merge(events_all, on=\"week\", how=\"left\")\n",
    "\n",
    "# Slice TEST rows\n",
    "test_rows = comb_feat[comb_feat[\"units_sold\"].isna()].copy()\n",
    "test_rows = test_rows.merge(test[[\"sku_id\",\"week\"]], on=[\"sku_id\",\"week\"], how=\"inner\")\n",
    "\n",
    "# 3) Build TEST matrices aligned with TRAIN columns\n",
    "num_cols = [\n",
    "    \"price\",\"promo_flag\",\"discount_pct\",\n",
    "    \"week_sin\",\"week_cos\",\n",
    "    \"lag1_units\",\"lag2_units\",\"lag3_units\",\n",
    "    \"roll3_mean_units\",\n",
    "    \"lag1_price\",\"price_change_pct\",\"lag1_promo\",\n",
    "    \"total_units_week_lag1\",\n",
    "    \"event_strength_pred\",\n",
    "]\n",
    "\n",
    "X_test_num = test_rows[num_cols].astype(float).fillna(0)\n",
    "\n",
    "# Recreate one-hots with TRAIN's categories (align columns)\n",
    "evt_te = pd.get_dummies(test_rows[\"event_strength_bucket\"].fillna(\"mild\"), prefix=\"evt\")\n",
    "evt_te = evt_te.reindex(columns=[c for c in X_train.columns if c.startswith(\"evt_\")], fill_value=0)\n",
    "\n",
    "# For category/brand, align to TRAIN's one-hot columns\n",
    "cat_cols = [c for c in X_train.columns if c.startswith(\"category_\")]\n",
    "br_cols  = [c for c in X_train.columns if c.startswith(\"brand_\")]\n",
    "\n",
    "def align_one_hot(series: pd.Series, ref_cols, prefix: str):\n",
    "    # build one-hot on test then reindex to TRAIN columns\n",
    "    oh = pd.get_dummies(series.astype(pd.CategoricalDtype(\n",
    "        categories=[c.replace(prefix + \"_\",\"\") for c in ref_cols]\n",
    "    )), prefix=prefix)\n",
    "    return oh.reindex(columns=ref_cols, fill_value=0)\n",
    "\n",
    "cat_te = align_one_hot(test_rows[\"category\"], cat_cols, prefix=\"category\")\n",
    "br_te  = align_one_hot(test_rows[\"brand\"],    br_cols,  prefix=\"brand\")\n",
    "\n",
    "# Final TEST matrix\n",
    "X_test = pd.concat([X_test_num.reset_index(drop=True),\n",
    "                    evt_te.reset_index(drop=True),\n",
    "                    cat_te.reset_index(drop=True),\n",
    "                    br_te.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Ensure the column order matches TRAIN exactly\n",
    "missing_in_test = [c for c in X_train.columns if c not in X_test.columns]\n",
    "for c in missing_in_test: X_test[c] = 0\n",
    "missing_in_train = [c for c in X_test.columns if c not in X_train.columns]\n",
    "for c in missing_in_train: X_train[c] = 0\n",
    "X_test = X_test[X_train.columns]\n",
    "\n",
    "X_train.shape, X_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      6\u001b[39m model = HistGradientBoostingRegressor(\n\u001b[32m      7\u001b[39m     learning_rate=\u001b[32m0.1\u001b[39m,\n\u001b[32m      8\u001b[39m     max_leaf_nodes=\u001b[32m63\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m     random_state=\u001b[32m42\u001b[39m\n\u001b[32m     13\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m model.fit(\u001b[43mX_train\u001b[49m, y_train)\n\u001b[32m     17\u001b[39m pred = np.clip(model.predict(X_test), \u001b[32m0\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     19\u001b[39m submission = test_rows[[\u001b[33m\"\u001b[39m\u001b[33msku_id\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mweek\u001b[39m\u001b[33m\"\u001b[39m]].copy()\n",
      "\u001b[31mNameError\u001b[39m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Cell D ‚Äî Train baseline model and create submission\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "import numpy as np\n",
    "\n",
    "model = HistGradientBoostingRegressor(\n",
    "    learning_rate=0.1,\n",
    "    max_leaf_nodes=63,\n",
    "    min_samples_leaf=30,\n",
    "    max_iter=400,\n",
    "    early_stopping=False,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "pred = np.clip(model.predict(X_test), 0, None)\n",
    "\n",
    "submission = test_rows[[\"sku_id\",\"week\"]].copy()\n",
    "submission[\"units_sold_next_week\"] = pred\n",
    "submission.to_csv(\"submission_baseline.csv\", index=False)\n",
    "\n",
    "submission.head(), (\"Saved to submission_baseline.csv\", submission.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MuWtPhSbtIoy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Using features: ['sku_id', 'week', 'price', 'promo_flag']\n",
      "ü§ñ Training Random Forest regressor...\n",
      "‚úÖ Predictions saved: 12000 predictions\n",
      "   Preview:    sku_id  week  units_sold_next_week\n",
      "0       1    45                 73.02\n",
      "1       1    46                 72.21\n",
      "2       1    47                 66.83\n",
      "   Prediction range: 4.38 to 113.81\n"
     ]
    }
   ],
   "source": [
    "# # 2. Tabular-Only Baseline Model and Predictions\n",
    "\n",
    "# # Select numeric features for baseline\n",
    "# numeric_features = ['sku_id', 'week', 'price', 'promo_flag']\n",
    "# print(f\"üìä Using features: {numeric_features}\")\n",
    "\n",
    "# # Prepare training data\n",
    "# X_train = train_sales[numeric_features]\n",
    "# y_train = train_sales['units_sold']  # Target variable\n",
    "\n",
    "# # Prepare test data\n",
    "# X_test = test_sales[numeric_features]\n",
    "\n",
    "# # Train simple Random Forest regressor baseline\n",
    "# print(\"ü§ñ Training Random Forest regressor...\")\n",
    "# model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions\n",
    "# predictions = model.predict(X_test)\n",
    "\n",
    "# # Create submission file\n",
    "# submission_df = pd.DataFrame({\n",
    "#     'sku_id': test_sales['sku_id'],\n",
    "#     'week': test_sales['week'],\n",
    "#     'units_sold_next_week': predictions\n",
    "# })\n",
    "\n",
    "# # Save predictions\n",
    "# submission_df.to_csv(\"commerce_challenge1_predictions.csv\", index=False)\n",
    "# print(f\"‚úÖ Predictions saved: {submission_df.shape[0]} predictions\")\n",
    "# print(f\"   Preview: {submission_df.head(3)}\")\n",
    "# print(f\"   Prediction range: {predictions.min():.2f} to {predictions.max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jECsPl18tIoz"
   },
   "outputs": [],
   "source": [
    "# # 3. Submit Predictions\n",
    "\n",
    "# # Submit predictions to the competition\n",
    "# print(\"üöÄ Submitting predictions...\")\n",
    "\n",
    "# try:\n",
    "#     result = client.submit_prediction(\"Commerce\", 1, \"commerce_challenge1_predictions.csv\")\n",
    "\n",
    "#     if result['success']:\n",
    "#         print(\"‚úÖ Submission successful!\")\n",
    "#         print(f\"   üìä Score: {result['score']:.4f}\")\n",
    "#         print(f\"   üìè Metric: {result['metric_name']}\")\n",
    "#         print(f\"   ‚úîÔ∏è  Validation: {'Passed' if result['validation_passed'] else 'Failed'}\")\n",
    "#     else:\n",
    "#         print(\"‚ùå Submission failed!\")\n",
    "#         print(f\"   Error details: {result.get('details', {}).get('validation_errors', 'Unknown error')}\")\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(f\"üí• Submission error: {e}\")\n",
    "#     print(\"üîß Check your API key and team name are correct!\")\n",
    "\n",
    "# print(\"\\nüéØ Next steps:\")\n",
    "# print(\"   1. Try incorporating relevant information outside this table!\")\n",
    "# print(\"   2. Move on to Commerce Challenge 2!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
